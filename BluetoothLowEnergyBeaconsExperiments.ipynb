{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with Bluetooth Low Energy Beacon Signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inline Matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "## Imports ##\n",
    "# JSON\n",
    "import json\n",
    "\n",
    "# NumPy\n",
    "import numpy as np\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "pd.set_option('display.min_rows', 250)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SciPy\n",
    "from scipy import stats\n",
    "\n",
    "# seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# scikit-learn\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant that indicated how long each captured beacon be considered to be present (value in milliseconds).\n",
    "# This effect can be simulated by a running average window over the collected data.\n",
    "beacons_inactivity_timer=5000\n",
    "# Constant that indicates how often the device reports about the surrounding beacons (value in milliseconds). \n",
    "# This effect can be simulated by resampling the collected data.  \n",
    "beacons_refresh_interval=1000\n",
    "\n",
    "# A function that receives the path to a JSON file containing a data collection run to load that file.\n",
    "# It also receives a few extra to append to the loaded data which characterize the objective of the loaded data, i.e.:\n",
    "# - The \"target beacons\" that we were interested in during the data collection phase.\n",
    "# - The distance that those target beacons were from the data collection point.\n",
    "def log_loader(file_path, target_beacons, distance):\n",
    "    with open(file_path) as json_file:\n",
    "        log = json.load(json_file)\n",
    "        log['target_beacons'] = target_beacons\n",
    "        log['distance'] = distance   \n",
    "        return log\n",
    "\n",
    "# This function receives a loaded of logs (see previous function), iterates over them, and unrolls them into a DataFrame.\n",
    "def convert_logs_to_dataframe(logs):\n",
    "    # List of per log intermediary DataFrames.\n",
    "    dfs = []\n",
    "    # Iterate over each log.\n",
    "    for log in logs:\n",
    "        # List of lists that represent a table that will be converted into a DataFrame.\n",
    "        table = []\n",
    "        # Iterate over the list of target beacons.\n",
    "        for beacon in log['target_beacons']:\n",
    "            # Iterate over the logging sessions contained within a log file.\n",
    "            for entries in log['sessions'].values():\n",
    "                # Iterate over the entries of each of those sessions.\n",
    "                for entry in entries:\n",
    "                    # Get each of the readings in the the entry.\n",
    "                    reading = entry['reading']\n",
    "                    # Check if the reading matches our target iBeacon.\n",
    "                    if reading['type'] == 'iBeacon' and reading['values'][0] == beacon['uuid']\\\n",
    "                    and reading['values'][1] == beacon['major'] and reading['values'][2] == beacon['minor']:\n",
    "                        # If so extract all the information into and append as row in the table\n",
    "                        table.append([log['name'],\n",
    "                                      pd.to_datetime(log['timestamp'], unit='ms'),\n",
    "                                      entry['id'],\n",
    "                                      reading['id'],\n",
    "                                      reading['type'],\n",
    "                                      str(reading['values']),\n",
    "                                      reading['values'][0],\n",
    "                                      reading['values'][1],\n",
    "                                      reading['values'][2],\n",
    "                                      reading['txPower'],\n",
    "                                      reading['rssi'],\n",
    "                                      pd.to_datetime(reading['timestamp'], unit='ms'),\n",
    "                                      reading['avgRssi'],\n",
    "                                      log['distance']])\n",
    "        \n",
    "        # After collection everything of interest in the log file into a plain \"list of lists\",\n",
    "        # convert into a DataFrame by specifying the column names\n",
    "        df = pd.DataFrame(table, columns=['filename',\n",
    "                                          'creationTimestamp',\n",
    "                                          'entry',\n",
    "                                          'id',\n",
    "                                          'type',\n",
    "                                          'values',\n",
    "                                          'uuid',\n",
    "                                          'major',\n",
    "                                          'minor',\n",
    "                                          'txPower',\n",
    "                                          'rssi',\n",
    "                                          'timestamp',\n",
    "                                          'avgRssi',\n",
    "                                          'distance'])\n",
    "        # If for some reason there are duplicate timestamps on a log file drop them.\n",
    "        df.drop_duplicates(subset=['timestamp'], inplace=True)\n",
    "        # Append the DataFrame to the list of DataFrames.\n",
    "        dfs.append(df)\n",
    "    # Return the single DataFrame that results from the concatenation of the individual DataFrames.\n",
    "    # Also reset the index and select only the relevant columns.\n",
    "    return pd.concat(dfs, ignore_index=True)[['timestamp','values', 'distance', 'rssi', 'avgRssi']]\n",
    "\n",
    "# A function that should be applied per each device/distance combo, assuming that this corresponds to one and only one of\n",
    "# the previously loaded log files.\n",
    "def per_group(d):\n",
    "    # Use the inverse of the zscore as the weight for each of the samples\n",
    "    d['weight'] = 1 / np.abs(stats.zscore(d['rssi']))\n",
    "    # However, the z-score result can be 0 so 1/0 will result in infinity. We need to replace those values by something else.\n",
    "    # For now, we simply indentify what is the largest value other than infinity and set all the instances of infinity by\n",
    "    # a number that is slightly larger than that (1% to be exact).\n",
    "    d.replace(np.inf, d.loc[d.weight != np.inf, 'weight'].max()*1.01, inplace=True)\n",
    "    # Calculate a running average over the RSSI values to filter out undesired signal strength fluctuation. \n",
    "    # In practive, this type of filtering needs to reach a trade-off between stopping abrupt signal changes and \n",
    "    # responsiveness to user moveiment.\n",
    "    d['rolling_mean_rssi'] = d[['timestamp', 'rssi']].rolling(pd.Timedelta(beacons_inactivity_timer, unit='ms'), on='timestamp')['rssi'].mean()\n",
    "    # Return the DataFrame slice to be merged with all the others.\n",
    "    return d\n",
    "\n",
    "def resample_group(d, sample_interval=beacons_refresh_interval):\n",
    "    r = d.set_index('timestamp', verify_integrity=True).resample(pd.Timedelta(sample_interval, unit='ms'), label='right', closed='right').pad()\n",
    "    r['timestamp_diff'] = r.index\n",
    "    r['timestamp_diff'] = r['timestamp_diff'].diff()\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# Generate the list of distances at which we collected samples (i.e., 0 to 10 meters in 0.5 meters intervals)\n",
    "distances = np.linspace(0,10,21)\n",
    "# A dictionary that describes our data collection task.\n",
    "collections = {\n",
    "    # A laptop\n",
    "    'laptop': {\n",
    "        # collected samples at theses distances\n",
    "        'distances': distances,\n",
    "        # from the following targets:\n",
    "        'targets': [{ 'uuid': '113069EC-6E64-4BD3-6810-DE01B36E8A3E', 'major': 1, 'minor': 102 }]\n",
    "    },\n",
    "    # A smartphone\n",
    "    'smartphone':  {\n",
    "        # collected samples at these distances\n",
    "        'distances': distances,\n",
    "        # from the following targets:\n",
    "        'targets': [{ 'uuid': '113069EC-6E64-4BD3-6810-DE01B36E8A3E', 'major': 1, 'minor': 101 }]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize a dictionary that will store the dataframes for each of the devices, indexed by the device name.\n",
    "tables = {}\n",
    "# Iterate over the data collection task defined above\n",
    "for name, device in collections.items():\n",
    "    # Initialize a list to collect the logs for each of the distances.\n",
    "    logs = []\n",
    "    # Iterate over each of the distances collected for the device.\n",
    "    for d in device['distances']:\n",
    "        # Load the corresponding file based on the following pattern\n",
    "        logs.append(log_loader('data/beacons/'+name+'/beacons-'+str(d)+'.json', device['targets'], d))\n",
    "    # Conver the list of logs to a DataFrame and story in the tables dictionary indexed by the device name.\n",
    "    tables[name] = convert_logs_to_dataframe(logs)\n",
    "    # Add a column to the DataFrame which identifies each of each lines as belonging to a certain device.\n",
    "    tables[name]['device'] = name\n",
    "\n",
    "# Concatenate the DataFrames for each device into a single DataFrame (recreate the index while at it).\n",
    "data = pd.concat(tables.values(), ignore_index=True)\n",
    "# Group the data by Device and Distance, apply the \"per_collector_device_distance\" function.\n",
    "# NOTE: This is needed to perform a few extra computations for each of the logging sessions. I could have probably done this\n",
    "# while loading the data but I decided to keep things separate. However, this may need to be changed if in the future\n",
    "# grouping turns out not to be enough to slice the data on \"per log\" basis.\n",
    "data = data.groupby(['device','distance']).apply(per_group).reset_index(drop=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file.\n",
    "data.to_csv('out/beacons.csv')\n",
    "# Save the DataFrame to an Excel file\n",
    "data.to_excel('out/beacons.xlsx')\n",
    "# Display the DataFrame\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_plot = data #[data.rssi_zscore < 3]\n",
    "data_plot_laptop = data_plot[data_plot.device == 'laptop']\n",
    "data_plot_smartphone = data_plot[data_plot.device == 'smartphone']\n",
    "\n",
    "X = data_plot['rolling_mean_rssi'].values.reshape(-1,1)\n",
    "X_weight = data_plot['weight']\n",
    "y = data_plot['distance'].values.reshape(-1,1)\n",
    "\n",
    "X_laptop = data_plot_laptop['rolling_mean_rssi'].values.reshape(-1,1)\n",
    "y_laptop = data_plot_laptop['distance'].values.reshape(-1,1)\n",
    "\n",
    "X_smartphone = data_plot_smartphone['rolling_mean_rssi'].values.reshape(-1,1)\n",
    "y_smartphone = data_plot_smartphone['distance'].values.reshape(-1,1)\n",
    "\n",
    "data_plot_smartphone_rssi_mean = data_plot_smartphone.groupby(['device','distance'])['distance','rssi'].mean()\n",
    "data_plot_laptop_rssi_mean = data_plot_laptop.groupby(['device','distance'])['distance','rssi'].mean()\n",
    "\n",
    "linear_regression_standard_scaler = StandardScaler()\n",
    "linear_regression = LinearRegression()\n",
    "\n",
    "weighted_linear_regression_standard_scaler = StandardScaler()\n",
    "weighted_linear_regression = LinearRegression()\n",
    "\n",
    "linear_regression_pipeline = Pipeline([\n",
    "    ('scaler', linear_regression_standard_scaler),\n",
    "    ('regression', linear_regression)\n",
    "])\n",
    "linear_regression_pipeline.fit(X, y)\n",
    "\n",
    "weighted_linear_regression_pipeline = Pipeline([\n",
    "    ('scaler', weighted_linear_regression_standard_scaler),\n",
    "    ('regression', weighted_linear_regression)\n",
    "])\n",
    "weighted_linear_regression_pipeline.fit(X, y, regression__sample_weight = X_weight)\n",
    "\n",
    "linear_regression_predictions = linear_regression_pipeline.predict(X)\n",
    "weighted_linear_regression_predictions = weighted_linear_regression_pipeline.predict(X)\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "\n",
    "plt.scatter(y_smartphone, X_smartphone, c='#ff00003f', label='Smartphone')\n",
    "plt.scatter(y_laptop, X_laptop,c='#00ff003f', label='Laptop')\n",
    "plt.scatter(data_plot_smartphone_rssi_mean.distance,\n",
    "            data_plot_smartphone_rssi_mean.rssi,\n",
    "            c='#ffff00', label='Smartphone RSSI Mean')\n",
    "plt.scatter(data_plot_laptop_rssi_mean.distance,\n",
    "            data_plot_laptop_rssi_mean.rssi,\n",
    "            c='#0000ff', label='Laptop RSSI Mean')\n",
    "\n",
    "plt.plot(linear_regression_predictions, X, color='red')\n",
    "plt.plot(weighted_linear_regression_predictions, X, color='blue')\n",
    "\n",
    "plt.xlim(min(y)-1, max(y)+1)\n",
    "plt.ylim(min(X)-1, max(X)+1)\n",
    "\n",
    "plt.xticks(np.arange(min(y), max(y)+1, 1.0))\n",
    "plt.yticks(np.arange(min(X), max(X)+1, 2.0))\n",
    "\n",
    "plt.xlabel('Distance (m)')\n",
    "plt.ylabel('RSSI (dB)')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right');\n",
    "\n",
    "plt.show()\n",
    "\n",
    "linear_regression_r2 = linear_regression.score(X, y)\n",
    "weighted_linear_regression_r2 = weighted_linear_regression.score(X, y, data_plot['weight'])\n",
    "# R2 Score\n",
    "print(\"Linear Regression R2 Score\", linear_regression_r2)\n",
    "print(\"Weighted Linear Regression R2 Score\", weighted_linear_regression_r2)\n",
    "\n",
    "# Equations\n",
    "print('Linear Regreation: y =',str(linear_regression.coef_[0][0])+'x +',linear_regression.intercept_[0])\n",
    "print('Weighted Linear Regreation: y =',str(weighted_linear_regression.coef_[0][0])+'x +',weighted_linear_regression.intercept_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data for Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 656\n",
    "y_labels = data['distance'].astype(str).values.reshape(-1,1)\n",
    "\n",
    "crossval = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state).split(X, y_labels)\n",
    "scorer = make_scorer(mean_absolute_error)\n",
    "linear_regression_scores = pd.DataFrame(cross_val_score(linear_regression_pipeline, X, y,\n",
    "                                                        cv=crossval, scoring=scorer),\n",
    "                                        columns=['scores'])\n",
    "print('Linear Regression')\n",
    "print(linear_regression_scores)\n",
    "print(linear_regression_scores.describe())\n",
    "\n",
    "crossval = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state).split(X, y_labels)\n",
    "weighted_linear_regression_scores = pd.DataFrame(cross_val_score(weighted_linear_regression_pipeline, X, y,\n",
    "                                                                 cv=crossval, scoring=scorer,\n",
    "                                                                 fit_params = { 'regression__sample_weight': X_weight }),\n",
    "                                        columns=['scores'])\n",
    "print('Weighted Linear Regression')\n",
    "print(weighted_linear_regression_scores)\n",
    "print(weighted_linear_regression_scores.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"General\" Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Using Seaborn Boxplot to get an overview of the whole distance vs. rssi correlation.\n",
    "# NOTE: I could have possibly made this with plain Matplotlib but Seaborn makes it prettier and easier.\n",
    "fig = plt.figure(figsize=(16,9))\n",
    "sns.boxplot(data['distance'], data['rssi'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSSI over time per distance and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_rssi = data['rssi'].min()\n",
    "max_rssi = data['rssi'].max()\n",
    "\n",
    "for k, d in data.groupby(['distance','device']):\n",
    "    dr = resample_group(d, 1000)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.title('Device: '+k[1]+' Distance: '+str(k[0])+' m')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('RSSI (dB)')\n",
    "    plt.plot(d['timestamp'], d['rssi'], label='RSSI', color='blue')\n",
    "    plt.plot(dr.index, dr['rolling_mean_rssi'], label='Rolling Mean RSSI', color='red')\n",
    "    plt.yticks(np.arange(min_rssi, max_rssi, 2.0))\n",
    "    plt.legend(loc='lower right');\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just resampling the data for future use (still nothing to do with it right now)\n",
    "data_resample = data.groupby(['device','distance']).apply(resample_group).reset_index(drop=True)\n",
    "# Display the result\n",
    "data_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
