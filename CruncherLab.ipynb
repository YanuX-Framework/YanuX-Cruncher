{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CruncherLab #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and set up the environment ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import getopt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from yanux.cruncher.model.loader import JsonLoader\n",
    "from yanux.cruncher.model.wifi import WifiLogs\n",
    "from yanux.cruncher.ml.experiments import *\n",
    "\n",
    "pd.set_option('display.max_rows', 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Input & Output Data Directories and other parameters ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data Directory is: data\n",
      "Output Data Directory is out\n"
     ]
    }
   ],
   "source": [
    "input_data_directory = \"data\"\n",
    "output_data_directory = \"out\"\n",
    "\n",
    "print(\"Input Data Directory is:\", input_data_directory)\n",
    "print(\"Output Data Directory is\", output_data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the output directory if it doesn't exist ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(output_data_directory):\n",
    "    os.makedirs(output_data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from the Input Data Directory ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_loader = JsonLoader(input_data_directory)\n",
    "wifi_logs = WifiLogs(json_loader.json_data)\n",
    "wifi_logs.shuffle_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the data into a Pandas Dataframe, in which each Wi-Fi result reading is represented by a single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wifi_results_columns = [\"filename\", \"x\", \"y\", \"floor\", \"orientation\", \"sample_id\", \"mac_address\",\n",
    "                        \"timestamp\", \"signal_strength\"]\n",
    "\n",
    "wifi_results = pd.DataFrame(wifi_logs.wifi_results(), columns=wifi_results_columns)\n",
    "wifi_results.to_csv(output_data_directory + \"/wifi_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the unique MAC Addresses present in the recorded data. Each one represents a single Wi-Fi Access Point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mac_addresses = wifi_results.mac_address.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, store the data into a Pandas Dataframe in which each line represents a single sampling cycle with *n* different readings for each of the Access Points within range. Those readings are stored as columns along each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wifi_samples_columns = [\"filename\", \"x\", \"y\", \"floor\", \"orientation\", \"sample_id\", \"timestamp\"]\n",
    "wifi_samples_columns.extend(mac_addresses)\n",
    "\n",
    "wifi_samples = pd.DataFrame(wifi_logs.wifi_samples(), columns=wifi_samples_columns)\n",
    "wifi_samples = wifi_samples.sort_values([\"filename\", \"x\", \"y\", \"floor\", \"sample_id\"]).reset_index(drop=True)\n",
    "wifi_samples.to_csv(output_data_directory + \"/wifi_samples.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Data Set ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *** TODO *** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Train and Test Scenario ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the train and test scenario generation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw = True\n",
    "groupby_mean = False\n",
    "groupby_max = False\n",
    "groupby_min = False\n",
    "data_partials = [0.35]\n",
    "test_data_partials = [0.35]\n",
    "filename_prefixes = [\"point\", \"altPoint\"]\n",
    "subset_locations_values = [0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Training and Test Scenarios...\n",
      "# Data Scenarios: 5\n",
      "# Test Data Scenarios: 4\n",
      "Scenarios Generated!\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating Training and Test Scenarios...\")\n",
    "\n",
    "data_scenarios = {}\n",
    "test_data_scenarios = {}\n",
    "\n",
    "full_data_scenarios = {}\n",
    "prepare_full_data_scenarios(wifi_samples, full_data_scenarios,\n",
    "                            raw=raw,\n",
    "                            groupby_mean=groupby_mean,\n",
    "                            groupby_max=groupby_max,\n",
    "                            groupby_min=groupby_min)\n",
    "\n",
    "full_test_data_scenarios = {}\n",
    "prepare_full_data_scenarios(wifi_samples, full_test_data_scenarios,\n",
    "                            raw=raw,\n",
    "                            groupby_mean=groupby_mean,\n",
    "                            groupby_max=groupby_max,\n",
    "                            groupby_min=groupby_min)\n",
    "\n",
    "data_scenarios.update(full_data_scenarios)\n",
    "test_data_scenarios.update(full_test_data_scenarios)\n",
    "\n",
    "partial_data_scenarios = {}\n",
    "prepare_partial_data_scenarios(wifi_samples, partial_data_scenarios,\n",
    "                               slice_at_the_end=False,\n",
    "                               raw=raw,\n",
    "                               groupby_mean=groupby_mean,\n",
    "                               groupby_max=groupby_max,\n",
    "                               groupby_min=groupby_min,\n",
    "                               partials=data_partials)\n",
    "partial_test_data_scenarios = {}\n",
    "prepare_partial_data_scenarios(wifi_samples, partial_test_data_scenarios,\n",
    "                               slice_at_the_end=True,\n",
    "                               raw=raw,\n",
    "                               groupby_mean=groupby_mean,\n",
    "                               groupby_max=groupby_max,\n",
    "                               groupby_min=groupby_min,\n",
    "                               partials=test_data_partials)\n",
    "\n",
    "data_scenarios.update(partial_data_scenarios)\n",
    "test_data_scenarios.update(partial_test_data_scenarios)\n",
    "\n",
    "filename_startswith_data_scenarios = {}\n",
    "for filename_prefix in filename_prefixes:\n",
    "    prepare_filename_startswith_data_scenarios(wifi_samples, filename_startswith_data_scenarios,\n",
    "                                               raw=raw,\n",
    "                                               groupby_mean=groupby_mean,\n",
    "                                               groupby_max=groupby_max,\n",
    "                                               groupby_min=groupby_min,\n",
    "                                               filename_startswith=filename_prefix)\n",
    "filename_startswith_test_data_scenarios = {}\n",
    "for filename_prefix in filename_prefixes:\n",
    "    prepare_filename_startswith_data_scenarios(wifi_samples, filename_startswith_test_data_scenarios,\n",
    "                                               raw=raw,\n",
    "                                               groupby_mean=groupby_mean,\n",
    "                                               groupby_max=groupby_max,\n",
    "                                               groupby_min=groupby_min,\n",
    "                                               filename_startswith=filename_prefix)\n",
    "\n",
    "data_scenarios.update(filename_startswith_data_scenarios)\n",
    "test_data_scenarios.update(filename_startswith_test_data_scenarios)\n",
    "\n",
    "subset_locations_data_scenarios = {}\n",
    "for subset_locations in subset_locations_values:\n",
    "    prepare_full_data_scenarios(subset_wifi_samples_locations(wifi_samples, subset_locations),\n",
    "                                subset_locations_data_scenarios,\n",
    "                                raw=raw,\n",
    "                                groupby_mean=groupby_mean,\n",
    "                                groupby_max=groupby_max,\n",
    "                                groupby_min=groupby_min,\n",
    "                                scenarios_suffix=\"subset_locations=\" + str(subset_locations))\n",
    "\n",
    "data_scenarios.update(subset_locations_data_scenarios)\n",
    "\n",
    "\n",
    "path_direction_aggregated_data_scenarios = {}\n",
    "prepare_path_direction_aggregated_data_scenarios(wifi_samples, path_direction_aggregated_data_scenarios,\n",
    "                                         groupby_mean=groupby_mean,\n",
    "                                         groupby_max=groupby_max,\n",
    "                                         groupby_min=groupby_min)\n",
    "\n",
    "data_scenarios.update(path_direction_aggregated_data_scenarios)\n",
    "\n",
    "\n",
    "save_scenarios(data_scenarios, output_directory=output_data_directory, prefix=\"train_\")\n",
    "print(\"# Data Scenarios: \" + str(len(data_scenarios)))\n",
    "save_scenarios(test_data_scenarios, output_directory=output_data_directory, prefix=\"test_\")\n",
    "print(\"# Test Data Scenarios: \" + str(len(test_data_scenarios)))\n",
    "\n",
    "print(\"Scenarios Generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### # Neighbors ###\n",
    "Test how the *k* value influences performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>std_dev_distance_error</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>percentile_25</th>\n",
       "      <th>percentile_50</th>\n",
       "      <th>percentile_75</th>\n",
       "      <th>percentile_90</th>\n",
       "      <th>percentile_95</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>2.325253</td>\n",
       "      <td>1.313599</td>\n",
       "      <td>7.130617</td>\n",
       "      <td>1.555556</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.111111</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>2.345407</td>\n",
       "      <td>1.308725</td>\n",
       "      <td>7.211980</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.142857</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2.362713</td>\n",
       "      <td>1.322088</td>\n",
       "      <td>7.328580</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2.400288</td>\n",
       "      <td>1.318256</td>\n",
       "      <td>7.497444</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.679340</td>\n",
       "      <td>1.347943</td>\n",
       "      <td>8.994000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k  mean_absolute_error  std_dev_distance_error  mean_squared_error  \\\n",
       "4  9             2.325253                1.313599            7.130617   \n",
       "3  7             2.345407                1.308725            7.211980   \n",
       "2  5             2.362713                1.322088            7.328580   \n",
       "1  3             2.400288                1.318256            7.497444   \n",
       "0  1             2.679340                1.347943            8.994000   \n",
       "\n",
       "   percentile_25  percentile_50  percentile_75  percentile_90  percentile_95  \\\n",
       "4       1.555556            2.0       3.111111            4.0       4.666667   \n",
       "3       1.428571            2.0       3.142857            4.0       4.571429   \n",
       "2       1.581139            2.0       3.200000            4.0       4.800000   \n",
       "1       2.000000            2.0       3.333333            4.0       4.666667   \n",
       "0       2.000000            2.0       4.000000            4.0       6.000000   \n",
       "\n",
       "        min       max  \n",
       "4  0.000000  9.555556  \n",
       "3  0.000000  9.428571  \n",
       "2  0.000000  9.200000  \n",
       "1  0.000000  9.333333  \n",
       "0  1.581139  8.000000  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neighbors=range(1,11,2)\n",
    "weights=\"uniform\"\n",
    "metric=\"euclidean\"\n",
    "nan_filler=-100\n",
    "\n",
    "curr_data = data_scenarios[\"full_data\"].fillna(nan_filler)\n",
    "curr_test_data = test_data_scenarios[\"full_data\"].fillna(nan_filler)\n",
    "\n",
    "# Just a metrics accumulator\n",
    "metrics = []\n",
    "for k in n_neighbors:\n",
    "    curr_metrics = experiment_metrics(knn_experiment(curr_data,\n",
    "                                                     mac_addresses,\n",
    "                                                     [\"x\", \"y\"],\n",
    "                                                     algorithm=\"brute\",\n",
    "                                                     n_neighbors=k,\n",
    "                                                     weights=weights,\n",
    "                                                     metric=metric,\n",
    "                                                     test_data=curr_test_data))\n",
    "    curr_metrics[\"k\"] = k\n",
    "    metrics.append(curr_metrics)\n",
    "\n",
    "cols = [\"k\"] + list(curr_metrics.keys())[:-1]\n",
    "metrics_table = pd.DataFrame(metrics, columns=cols)\n",
    "metrics_table.to_csv(\"metrics-n_neighbors.csv\")\n",
    "metrics_table.sort_values(cols[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights ###\n",
    "Check whether the neighbors should have the same (*uniform*) or a weighted (*distance*-based) influence in the regression result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>weights</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>std_dev_distance_error</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>percentile_25</th>\n",
       "      <th>percentile_50</th>\n",
       "      <th>percentile_75</th>\n",
       "      <th>percentile_90</th>\n",
       "      <th>percentile_95</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>2.361716</td>\n",
       "      <td>1.318441</td>\n",
       "      <td>7.314252</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.186078</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.816928</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>9.204335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>uniform</td>\n",
       "      <td>2.362713</td>\n",
       "      <td>1.322088</td>\n",
       "      <td>7.328580</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>uniform</td>\n",
       "      <td>2.383901</td>\n",
       "      <td>1.326683</td>\n",
       "      <td>7.441313</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>distance</td>\n",
       "      <td>2.384371</td>\n",
       "      <td>1.317142</td>\n",
       "      <td>7.418354</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.015086</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.995867</td>\n",
       "      <td>0.012303</td>\n",
       "      <td>9.488821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>2.399842</td>\n",
       "      <td>1.314283</td>\n",
       "      <td>7.484855</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.219515</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.731515</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>9.326577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>uniform</td>\n",
       "      <td>2.400288</td>\n",
       "      <td>1.318256</td>\n",
       "      <td>7.497444</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>uniform</td>\n",
       "      <td>2.428117</td>\n",
       "      <td>1.382673</td>\n",
       "      <td>7.805625</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>2.433114</td>\n",
       "      <td>1.374071</td>\n",
       "      <td>7.806227</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.014315</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.019448</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>8.999130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k   weights  mean_absolute_error  std_dev_distance_error  \\\n",
       "7  5  distance             2.361716                1.318441   \n",
       "6  5   uniform             2.362713                1.322088   \n",
       "4  4   uniform             2.383901                1.326683   \n",
       "5  4  distance             2.384371                1.317142   \n",
       "3  3  distance             2.399842                1.314283   \n",
       "2  3   uniform             2.400288                1.318256   \n",
       "0  2   uniform             2.428117                1.382673   \n",
       "1  2  distance             2.433114                1.374071   \n",
       "\n",
       "   mean_squared_error  percentile_25  percentile_50  percentile_75  \\\n",
       "7            7.314252       1.581139            2.0       3.186078   \n",
       "6            7.328580       1.581139            2.0       3.200000   \n",
       "4            7.441313       1.581139            2.0       3.000000   \n",
       "5            7.418354       1.581139            2.0       3.015086   \n",
       "3            7.484855       2.000000            2.0       3.219515   \n",
       "2            7.497444       2.000000            2.0       3.333333   \n",
       "0            7.805625       2.000000            2.0       3.000000   \n",
       "1            7.806227       2.000000            2.0       3.014315   \n",
       "\n",
       "   percentile_90  percentile_95       min       max  \n",
       "7            4.0       4.816928  0.001617  9.204335  \n",
       "6            4.0       4.800000  0.000000  9.200000  \n",
       "4            4.0       5.000000  0.000000  9.500000  \n",
       "5            4.0       4.995867  0.012303  9.488821  \n",
       "3            4.0       4.731515  0.004412  9.326577  \n",
       "2            4.0       4.666667  0.000000  9.333333  \n",
       "0            4.0       5.000000  0.000000  9.000000  \n",
       "1            4.0       5.019448  0.001178  8.999130  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neighbors=range(2,6,1)\n",
    "weights=[\"uniform\", \"distance\"]\n",
    "metric=\"euclidean\"\n",
    "nan_filler=-100\n",
    "\n",
    "curr_data = data_scenarios[\"full_data\"].fillna(nan_filler)\n",
    "curr_test_data = test_data_scenarios[\"full_data\"].fillna(nan_filler)\n",
    "\n",
    "# Just a metrics accumulator\n",
    "metrics = []\n",
    "for k in n_neighbors:\n",
    "    for w in weights:\n",
    "        curr_metrics = experiment_metrics(knn_experiment(curr_data,\n",
    "                                                         mac_addresses,\n",
    "                                                         [\"x\", \"y\"],\n",
    "                                                         algorithm=\"brute\",\n",
    "                                                         n_neighbors=k,\n",
    "                                                         weights=w,\n",
    "                                                         metric=metric,\n",
    "                                                         test_data=curr_test_data))\n",
    "        curr_metrics[\"k\"] = k\n",
    "        curr_metrics[\"weights\"] = w\n",
    "        metrics.append(curr_metrics)\n",
    "\n",
    "cols = [\"k\",\"weights\"] + list(curr_metrics.keys())[:-2]\n",
    "metrics_table = pd.DataFrame(metrics, columns=cols)\n",
    "metrics_table.to_csv(\"metrics-weights.csv\")\n",
    "metrics_table.sort_values(cols[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric ###\n",
    "Just test a few different distance metrics to assess if there is a better alternative than the plain old *euclidean* distance. The tested metrics include:\n",
    "- Euclidean Distance\n",
    "    - sqrt(sum((x - y)^2))\n",
    "- Manhattan Distance\n",
    "    - sum(|x - y|) \n",
    "- Chebyshev Distance\n",
    "    - sum(max(|x - y|))\n",
    "- Hamming Distance\n",
    "    - N_unequal(x, y) / N_tot\n",
    "- Canberra Distance\n",
    "    - sum(|x - y| / (|x| + |y|))\n",
    "- Braycurtis Similarity\n",
    "    - sum(|x - y|) / (sum(|x|) + sum(|y|))\n",
    "- S Euclidean Distance\n",
    "    - sqrt(sum((x - y)^2 / V))\n",
    "- Mahalanobis Distance\n",
    "    - sqrt((x - y)' V^-1 (x - y))\n",
    "\n",
    "The possible arguments are the following:\n",
    "- p = The order of the norm of the difference\n",
    "- V = array_like symmetric positive-definite covariance matrix.\n",
    "- w = (N,) array_like weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>std_dev_distance_error</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>percentile_25</th>\n",
       "      <th>percentile_50</th>\n",
       "      <th>percentile_75</th>\n",
       "      <th>percentile_90</th>\n",
       "      <th>percentile_95</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>manhattan</td>\n",
       "      <td>2.343701</td>\n",
       "      <td>1.345439</td>\n",
       "      <td>7.301333</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>braycurtis</td>\n",
       "      <td>2.349035</td>\n",
       "      <td>1.352972</td>\n",
       "      <td>7.346667</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>seuclidean</td>\n",
       "      <td>2.398519</td>\n",
       "      <td>1.284537</td>\n",
       "      <td>7.401278</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>euclidean</td>\n",
       "      <td>2.400288</td>\n",
       "      <td>1.318256</td>\n",
       "      <td>7.497444</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>canberra</td>\n",
       "      <td>2.403883</td>\n",
       "      <td>1.429142</td>\n",
       "      <td>7.819056</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chebyshev</td>\n",
       "      <td>2.819778</td>\n",
       "      <td>1.867725</td>\n",
       "      <td>11.436056</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.613029</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hamming</td>\n",
       "      <td>3.000341</td>\n",
       "      <td>2.160378</td>\n",
       "      <td>13.664611</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mahalanobis</td>\n",
       "      <td>3.770477</td>\n",
       "      <td>3.839462</td>\n",
       "      <td>28.943222</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>10.021749</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.941787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        metric  mean_absolute_error  std_dev_distance_error  \\\n",
       "1    manhattan             2.343701                1.345439   \n",
       "5   braycurtis             2.349035                1.352972   \n",
       "6   seuclidean             2.398519                1.284537   \n",
       "0    euclidean             2.400288                1.318256   \n",
       "4     canberra             2.403883                1.429142   \n",
       "2    chebyshev             2.819778                1.867725   \n",
       "3      hamming             3.000341                2.160378   \n",
       "7  mahalanobis             3.770477                3.839462   \n",
       "\n",
       "   mean_squared_error  percentile_25  percentile_50  percentile_75  \\\n",
       "1            7.301333       1.581139       2.000000       3.333333   \n",
       "5            7.346667       1.581139       2.000000       3.333333   \n",
       "6            7.401278       2.000000       2.000000       3.333333   \n",
       "0            7.497444       2.000000       2.000000       3.333333   \n",
       "4            7.819056       2.000000       2.000000       3.333333   \n",
       "2           11.436056       2.000000       2.000000       3.613029   \n",
       "3           13.664611       1.581139       2.666667       4.000000   \n",
       "7           28.943222       2.000000       2.666667       4.666667   \n",
       "\n",
       "   percentile_90  percentile_95  min        max  \n",
       "1       4.000000       4.666667  0.0   8.666667  \n",
       "5       4.000000       4.666667  0.0   8.666667  \n",
       "6       4.000000       4.666667  0.0   8.666667  \n",
       "0       4.000000       4.666667  0.0   9.333333  \n",
       "4       4.000000       5.333333  0.0  10.000000  \n",
       "2       5.333333       6.000000  0.0  12.000000  \n",
       "3       6.000000       7.333333  0.0  17.333333  \n",
       "7       6.666667      10.021749  0.0  31.941787  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neighbors=3\n",
    "weights=\"uniform\"\n",
    "metric=[\"euclidean\",\"manhattan\", \"chebyshev\",\n",
    "        \"hamming\", \"canberra\", \"braycurtis\",\n",
    "        \"seuclidean\", \"mahalanobis\"]\n",
    "nan_filler=-100\n",
    "\n",
    "curr_data = data_scenarios[\"full_data\"].fillna(nan_filler)\n",
    "curr_test_data = test_data_scenarios[\"full_data\"].fillna(nan_filler)\n",
    "\n",
    "# Just a metrics accumulator\n",
    "metrics = []\n",
    "for m in metric:\n",
    "    if metric in [\"mahalanobis\", \"seuclidean\"]:\n",
    "        metric_params = {'V': np.cov(curr_data[mac_addresses])}\n",
    "    else:\n",
    "        metric_params = None\n",
    "    curr_metrics = experiment_metrics(knn_experiment(curr_data,\n",
    "                                                     mac_addresses,\n",
    "                                                     [\"x\", \"y\"],\n",
    "                                                     algorithm=\"brute\",\n",
    "                                                     n_neighbors=n_neighbors,\n",
    "                                                     weights=weights,\n",
    "                                                     metric=m,\n",
    "                                                     metric_params=metric_params,\n",
    "                                                     test_data=curr_test_data))\n",
    "    curr_metrics[\"metric\"] = m\n",
    "    metrics.append(curr_metrics)\n",
    "\n",
    "cols = [\"metric\"] + list(curr_metrics.keys())[:-1]\n",
    "metrics_table = pd.DataFrame(metrics, columns=cols)\n",
    "metrics_table.to_csv(\"metrics-metric.csv\")\n",
    "metrics_table.sort_values(cols[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN filler values ###\n",
    "\n",
    "Test which is the signal strength value that should be considered for Access Points that are currently out of range. This is needed as part of the process of computing the distance/similarity between different fingerprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nan_filler</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>std_dev_distance_error</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>percentile_25</th>\n",
       "      <th>percentile_50</th>\n",
       "      <th>percentile_75</th>\n",
       "      <th>percentile_90</th>\n",
       "      <th>percentile_95</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-93.0</td>\n",
       "      <td>2.327970</td>\n",
       "      <td>1.289616</td>\n",
       "      <td>7.080889</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-100.0</td>\n",
       "      <td>2.400288</td>\n",
       "      <td>1.318256</td>\n",
       "      <td>7.497444</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1000000.0</td>\n",
       "      <td>2.666306</td>\n",
       "      <td>1.488223</td>\n",
       "      <td>9.321778</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.887301</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.673226</td>\n",
       "      <td>1.552882</td>\n",
       "      <td>9.555167</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.413666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2.692669</td>\n",
       "      <td>1.590058</td>\n",
       "      <td>9.776222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.794537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>2.733955</td>\n",
       "      <td>1.640245</td>\n",
       "      <td>10.162222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.399346</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.794537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nan_filler  mean_absolute_error  std_dev_distance_error  \\\n",
       "5       -93.0             2.327970                1.289616   \n",
       "1      -100.0             2.400288                1.318256   \n",
       "0  -1000000.0             2.666306                1.488223   \n",
       "2         0.0             2.673226                1.552882   \n",
       "3       100.0             2.692669                1.590058   \n",
       "4   1000000.0             2.733955                1.640245   \n",
       "\n",
       "   mean_squared_error  percentile_25  percentile_50  percentile_75  \\\n",
       "5            7.080889            2.0            2.0       2.666667   \n",
       "1            7.497444            2.0            2.0       3.333333   \n",
       "0            9.321778            2.0            2.0       3.887301   \n",
       "2            9.555167            2.0            2.0       3.333333   \n",
       "3            9.776222            2.0            2.0       3.333333   \n",
       "4           10.162222            2.0            2.0       3.399346   \n",
       "\n",
       "   percentile_90  percentile_95  min        max  \n",
       "5       4.000000       4.666667  0.0   9.333333  \n",
       "1       4.000000       4.666667  0.0   9.333333  \n",
       "0       4.666667       6.000000  0.0   9.333333  \n",
       "2       4.666667       6.000000  0.0  10.413666  \n",
       "3       4.666667       6.000000  0.0  11.794537  \n",
       "4       4.666667       6.000000  0.0  11.794537  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neighbors=3\n",
    "weights=\"uniform\"\n",
    "metric=\"euclidean\"\n",
    "nan_filler = [-1000000, -100, 0, 100, 1000000,\n",
    "              data_scenarios[\"full_data\"][mac_addresses].min().min()-1] \n",
    "\n",
    "# Just a metrics accumulator\n",
    "metrics = []\n",
    "for nf in nan_filler:\n",
    "    curr_data = data_scenarios[\"full_data\"].fillna(nf)\n",
    "    curr_test_data = test_data_scenarios[\"full_data\"].fillna(nf)\n",
    "    curr_metrics = experiment_metrics(knn_experiment(curr_data,\n",
    "                                                     mac_addresses,\n",
    "                                                     [\"x\", \"y\"],\n",
    "                                                     algorithm=\"brute\",\n",
    "                                                     n_neighbors=n_neighbors,\n",
    "                                                     weights=weights,\n",
    "                                                     metric=metric,\n",
    "                                                     test_data=curr_test_data))\n",
    "    curr_metrics[\"nan_filler\"] = nf\n",
    "    metrics.append(curr_metrics)\n",
    "\n",
    "cols = [\"nan_filler\"] + list(curr_metrics.keys())[:-1]\n",
    "metrics_table = pd.DataFrame(metrics, columns=cols)\n",
    "metrics_table.to_csv(\"metrics-nan_filler.csv\")\n",
    "metrics_table.sort_values(cols[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Units ###\n",
    "- dBm\n",
    "- mW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>std_dev_distance_error</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>percentile_25</th>\n",
       "      <th>percentile_50</th>\n",
       "      <th>percentile_75</th>\n",
       "      <th>percentile_90</th>\n",
       "      <th>percentile_95</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dBm</td>\n",
       "      <td>2.400288</td>\n",
       "      <td>1.318256</td>\n",
       "      <td>7.497444</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mW</td>\n",
       "      <td>4.311673</td>\n",
       "      <td>3.120495</td>\n",
       "      <td>28.318278</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.769375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  units  mean_absolute_error  std_dev_distance_error  mean_squared_error  \\\n",
       "0   dBm             2.400288                1.318256            7.497444   \n",
       "1    mW             4.311673                3.120495           28.318278   \n",
       "\n",
       "   percentile_25  percentile_50  percentile_75  percentile_90  percentile_95  \\\n",
       "0            2.0       2.000000       3.333333       4.000000       4.666667   \n",
       "1            2.0       3.333333       6.000000       9.333333      10.000000   \n",
       "\n",
       "   min        max  \n",
       "0  0.0   9.333333  \n",
       "1  0.0  22.769375  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neighbors=3\n",
    "weights=\"uniform\"\n",
    "metric=\"euclidean\"\n",
    "nan_filler=-100\n",
    "\n",
    "# Just a metrics accumulator\n",
    "metrics = []\n",
    "\n",
    "# Use the directly measured dBm values\n",
    "curr_data = data_scenarios[\"full_data\"].fillna(nan_filler)\n",
    "curr_test_data = test_data_scenarios[\"full_data\"].fillna(nan_filler)\n",
    "curr_metrics = experiment_metrics(knn_experiment(curr_data,\n",
    "                                                 mac_addresses,\n",
    "                                                 [\"x\", \"y\"],\n",
    "                                                 algorithm=\"brute\",\n",
    "                                                 n_neighbors=n_neighbors,\n",
    "                                                 weights=weights,\n",
    "                                                 metric=metric,\n",
    "                                                 test_data=curr_test_data))\n",
    "curr_metrics[\"units\"] = \"dBm\"\n",
    "metrics.append(curr_metrics)\n",
    "\n",
    "# Convert to mW\n",
    "curr_data[mac_addresses] = convert_to_units(curr_data[mac_addresses], from_units=\"dBm\", to_units=\"mW\")\n",
    "curr_test_data[mac_addresses] = convert_to_units(curr_test_data[mac_addresses], from_units=\"dBm\", to_units=\"mW\")\n",
    "curr_metrics = experiment_metrics(knn_experiment(curr_data,\n",
    "                                                 mac_addresses,\n",
    "                                                 [\"x\", \"y\"],\n",
    "                                                 algorithm=\"brute\",\n",
    "                                                 n_neighbors=n_neighbors,\n",
    "                                                 weights=weights,\n",
    "                                                 metric=metric,\n",
    "                                                 test_data=curr_test_data))\n",
    "curr_metrics[\"units\"] = \"mW\"\n",
    "metrics.append(curr_metrics)\n",
    "\n",
    "    \n",
    "cols = [\"units\"] + list(curr_metrics.keys())[:-1]\n",
    "metrics_table = pd.DataFrame(metrics, columns=cols)\n",
    "metrics_table.to_csv(\"metrics-units.csv\")\n",
    "metrics_table.sort_values(cols[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler ###\n",
    "Test different data scaling and normalization approaches to find out if any of them provides a clear advantage over the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaler</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>std_dev_distance_error</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>percentile_25</th>\n",
       "      <th>percentile_50</th>\n",
       "      <th>percentile_75</th>\n",
       "      <th>percentile_90</th>\n",
       "      <th>percentile_95</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>2.347341</td>\n",
       "      <td>1.292369</td>\n",
       "      <td>7.178556</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NormalizerEuclidean</td>\n",
       "      <td>2.398066</td>\n",
       "      <td>1.360154</td>\n",
       "      <td>7.598889</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>2.398519</td>\n",
       "      <td>1.284537</td>\n",
       "      <td>7.401278</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>2.400288</td>\n",
       "      <td>1.318256</td>\n",
       "      <td>7.497444</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NormalizerManhattan</td>\n",
       "      <td>2.402366</td>\n",
       "      <td>1.352824</td>\n",
       "      <td>7.599667</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>2.458955</td>\n",
       "      <td>1.382110</td>\n",
       "      <td>7.954778</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.027730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                scaler  mean_absolute_error  std_dev_distance_error  \\\n",
       "2         MinMaxScaler             2.347341                1.292369   \n",
       "3  NormalizerEuclidean             2.398066                1.360154   \n",
       "1       StandardScaler             2.398519                1.284537   \n",
       "0                 None             2.400288                1.318256   \n",
       "5  NormalizerManhattan             2.402366                1.352824   \n",
       "4         RobustScaler             2.458955                1.382110   \n",
       "\n",
       "   mean_squared_error  percentile_25  percentile_50  percentile_75  \\\n",
       "2            7.178556            2.0            2.0       3.333333   \n",
       "3            7.598889            2.0            2.0       3.333333   \n",
       "1            7.401278            2.0            2.0       3.333333   \n",
       "0            7.497444            2.0            2.0       3.333333   \n",
       "5            7.599667            2.0            2.0       3.333333   \n",
       "4            7.954778            2.0            2.0       3.333333   \n",
       "\n",
       "   percentile_90  percentile_95  min        max  \n",
       "2            4.0       4.666667  0.0  10.000000  \n",
       "3            4.0       4.666667  0.0  10.000000  \n",
       "1            4.0       4.666667  0.0   8.666667  \n",
       "0            4.0       4.666667  0.0   9.333333  \n",
       "5            4.0       4.666667  0.0  10.000000  \n",
       "4            4.0       5.333333  0.0   8.027730  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neighbors=3\n",
    "weights=\"uniform\"\n",
    "metric=\"euclidean\"\n",
    "nan_filler = -100\n",
    "\n",
    "scaler_values = {\"None\": None,\n",
    "                 \"MinMaxScaler\": preprocessing.MinMaxScaler(),\n",
    "                 \"StandardScaler\": preprocessing.StandardScaler(),\n",
    "                 \"RobustScaler\": preprocessing.RobustScaler(),\n",
    "                 \"NormalizerEuclidean\": preprocessing.Normalizer(norm=\"l2\"),\n",
    "                 \"NormalizerManhattan\": preprocessing.Normalizer(norm=\"l1\")}\n",
    "\n",
    "\n",
    "\n",
    "# Just a metrics accumulator\n",
    "metrics = []\n",
    "\n",
    "for scaler_name, scaler in scaler_values.items():\n",
    "    curr_data = data_scenarios[\"full_data\"].fillna(nan_filler)\n",
    "    curr_test_data = test_data_scenarios[\"full_data\"].fillna(nan_filler)\n",
    "    if scaler is not None:\n",
    "        scaler.fit(curr_data[mac_addresses])\n",
    "        curr_data[mac_addresses] = pd.DataFrame(scaler.transform(curr_data[mac_addresses]), columns=mac_addresses)\n",
    "        curr_test_data[mac_addresses] = pd.DataFrame(scaler.transform(curr_test_data[mac_addresses]), columns=mac_addresses)\n",
    "    \n",
    "    curr_metrics = experiment_metrics(knn_experiment(curr_data,\n",
    "                                                     mac_addresses,\n",
    "                                                     [\"x\", \"y\"],\n",
    "                                                     algorithm=\"brute\",\n",
    "                                                     n_neighbors=n_neighbors,\n",
    "                                                     weights=weights,\n",
    "                                                     metric=metric,\n",
    "                                                     metric_params=metric_params,\n",
    "                                                     test_data=curr_test_data))\n",
    "    curr_metrics[\"scaler\"] = scaler_name\n",
    "    metrics.append(curr_metrics)\n",
    "\n",
    "cols = [\"scaler\"] + list(curr_metrics.keys())[:-1]\n",
    "metrics_table = pd.DataFrame(metrics, columns=cols)\n",
    "metrics_table.to_csv(\"metrics-scaler.csv\")\n",
    "metrics_table.sort_values(cols[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Data Processing and Aggregation Scenarios ###\n",
    "\n",
    "Testing different ways of processing offline and online data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does the amount of retrieved online data before affects positioning performance while considering the full offline data? And also, how do different agrregation strategies (i.e., mean, maximum and minimum signal strength per location) affect the results? ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_scenario</th>\n",
       "      <th>test_data_scenario</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>std_dev_distance_error</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>percentile_25</th>\n",
       "      <th>percentile_50</th>\n",
       "      <th>percentile_75</th>\n",
       "      <th>percentile_90</th>\n",
       "      <th>percentile_95</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>full_data</td>\n",
       "      <td>partial_data_fraction=0.35</td>\n",
       "      <td>2.360641</td>\n",
       "      <td>1.391888</td>\n",
       "      <td>7.504444</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>full_data</td>\n",
       "      <td>full_data</td>\n",
       "      <td>2.400288</td>\n",
       "      <td>1.318256</td>\n",
       "      <td>7.497444</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data_scenario          test_data_scenario  mean_absolute_error  \\\n",
       "0     full_data  partial_data_fraction=0.35             2.360641   \n",
       "1     full_data                   full_data             2.400288   \n",
       "\n",
       "   std_dev_distance_error  mean_squared_error  percentile_25  percentile_50  \\\n",
       "0                1.391888            7.504444            2.0            2.0   \n",
       "1                1.318256            7.497444            2.0            2.0   \n",
       "\n",
       "   percentile_75  percentile_90  percentile_95  min       max  \n",
       "0       2.666667            4.0       5.333333  0.0  9.333333  \n",
       "1       3.333333            4.0       4.666667  0.0  9.333333  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neighbors=3\n",
    "weights=\"uniform\"\n",
    "metric=\"euclidean\"\n",
    "nan_filler=-100\n",
    "\n",
    "curr_data_scenarios = {}\n",
    "curr_data_scenarios.update(full_data_scenarios)\n",
    "\n",
    "curr_test_data_scenarios = {}\n",
    "curr_test_data_scenarios.update(full_test_data_scenarios)\n",
    "curr_test_data_scenarios.update(partial_test_data_scenarios)\n",
    "\n",
    "# Just a metrics accumulator\n",
    "metrics = []\n",
    "for data_scenario_name, data_scenario in curr_data_scenarios.items():\n",
    "    for test_data_scenario_name, test_data_scenario in curr_test_data_scenarios.items():\n",
    "        curr_data = data_scenario.fillna(nan_filler)\n",
    "        curr_test_data = test_data_scenario.fillna(nan_filler)\n",
    "        curr_metrics = experiment_metrics(knn_experiment(curr_data,\n",
    "                                                         mac_addresses,\n",
    "                                                         [\"x\", \"y\"],\n",
    "                                                         algorithm=\"brute\",\n",
    "                                                         n_neighbors=n_neighbors,\n",
    "                                                         weights=weights,\n",
    "                                                         metric=metric,\n",
    "                                                         test_data=curr_test_data))\n",
    "        curr_metrics[\"data_scenario\"] = data_scenario_name\n",
    "        curr_metrics[\"test_data_scenario\"] = test_data_scenario_name\n",
    "        metrics.append(curr_metrics)\n",
    "\n",
    "cols = [\"data_scenario\", \"test_data_scenario\"] + list(curr_metrics.keys())[:-2]\n",
    "metrics_table = pd.DataFrame(metrics, columns=cols)\n",
    "metrics_table.to_csv(\"metrics-data_scenarios.csv\")\n",
    "metrics_table.sort_values(cols[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does a varying number of samples per location on the offline data affects positioning performance? ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_scenario</th>\n",
       "      <th>test_data_scenario</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>std_dev_distance_error</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>percentile_25</th>\n",
       "      <th>percentile_50</th>\n",
       "      <th>percentile_75</th>\n",
       "      <th>percentile_90</th>\n",
       "      <th>percentile_95</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>full_data</td>\n",
       "      <td>full_data</td>\n",
       "      <td>2.400288</td>\n",
       "      <td>1.318256</td>\n",
       "      <td>7.497444</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partial_data_fraction=0.35</td>\n",
       "      <td>full_data</td>\n",
       "      <td>2.530294</td>\n",
       "      <td>1.459231</td>\n",
       "      <td>8.529611</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                data_scenario test_data_scenario  mean_absolute_error  \\\n",
       "1                   full_data          full_data             2.400288   \n",
       "0  partial_data_fraction=0.35          full_data             2.530294   \n",
       "\n",
       "   std_dev_distance_error  mean_squared_error  percentile_25  percentile_50  \\\n",
       "1                1.318256            7.497444            2.0            2.0   \n",
       "0                1.459231            8.529611            2.0            2.0   \n",
       "\n",
       "   percentile_75  percentile_90  percentile_95  min       max  \n",
       "1       3.333333            4.0       4.666667  0.0  9.333333  \n",
       "0       3.333333            4.0       6.000000  0.0  8.666667  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neighbors=3\n",
    "weights=\"uniform\"\n",
    "metric=\"euclidean\"\n",
    "nan_filler=-100\n",
    "\n",
    "curr_data_scenarios = {}\n",
    "curr_data_scenarios.update(full_data_scenarios)\n",
    "curr_data_scenarios.update(partial_data_scenarios)\n",
    "\n",
    "curr_test_data_scenarios = {}\n",
    "curr_test_data_scenarios.update(full_test_data_scenarios)\n",
    "\n",
    "# Just a metrics accumulator\n",
    "metrics = []\n",
    "for data_scenario_name, data_scenario in curr_data_scenarios.items():\n",
    "    for test_data_scenario_name, test_data_scenario in curr_test_data_scenarios.items():\n",
    "        curr_data = data_scenario.fillna(nan_filler)\n",
    "        curr_test_data = test_data_scenario.fillna(nan_filler)\n",
    "        curr_metrics = experiment_metrics(knn_experiment(curr_data,\n",
    "                                                         mac_addresses,\n",
    "                                                         [\"x\", \"y\"],\n",
    "                                                         algorithm=\"brute\",\n",
    "                                                         n_neighbors=n_neighbors,\n",
    "                                                         weights=weights,\n",
    "                                                         metric=metric,\n",
    "                                                         test_data=curr_test_data))\n",
    "        curr_metrics[\"data_scenario\"] = data_scenario_name\n",
    "        curr_metrics[\"test_data_scenario\"] = test_data_scenario_name\n",
    "        metrics.append(curr_metrics)\n",
    "\n",
    "cols = [\"data_scenario\", \"test_data_scenario\"] + list(curr_metrics.keys())[:-2]\n",
    "metrics_table = pd.DataFrame(metrics, columns=cols)\n",
    "metrics_table.to_csv(\"metrics-data_scenarios.csv\")\n",
    "metrics_table.sort_values(cols[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does orientation affect the positioning performance?  ####\n",
    "Comparison between data collected when moving through the floor plan from left to right, top to bottom, and when moving in the opposite directions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_scenario</th>\n",
       "      <th>test_data_scenario</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>std_dev_distance_error</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>percentile_25</th>\n",
       "      <th>percentile_50</th>\n",
       "      <th>percentile_75</th>\n",
       "      <th>percentile_90</th>\n",
       "      <th>percentile_95</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>filename_startswith_data_point</td>\n",
       "      <td>filename_startswith_data_point</td>\n",
       "      <td>2.203274</td>\n",
       "      <td>1.201214</td>\n",
       "      <td>6.294444</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filename_startswith_data_altPoint</td>\n",
       "      <td>filename_startswith_data_altPoint</td>\n",
       "      <td>2.462262</td>\n",
       "      <td>1.420160</td>\n",
       "      <td>8.075556</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filename_startswith_data_altPoint</td>\n",
       "      <td>filename_startswith_data_point</td>\n",
       "      <td>2.602709</td>\n",
       "      <td>1.545212</td>\n",
       "      <td>9.157000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>filename_startswith_data_point</td>\n",
       "      <td>filename_startswith_data_altPoint</td>\n",
       "      <td>2.707180</td>\n",
       "      <td>1.527109</td>\n",
       "      <td>9.656222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       data_scenario                 test_data_scenario  \\\n",
       "0     filename_startswith_data_point     filename_startswith_data_point   \n",
       "3  filename_startswith_data_altPoint  filename_startswith_data_altPoint   \n",
       "2  filename_startswith_data_altPoint     filename_startswith_data_point   \n",
       "1     filename_startswith_data_point  filename_startswith_data_altPoint   \n",
       "\n",
       "   mean_absolute_error  std_dev_distance_error  mean_squared_error  \\\n",
       "0             2.203274                1.201214            6.294444   \n",
       "3             2.462262                1.420160            8.075556   \n",
       "2             2.602709                1.545212            9.157000   \n",
       "1             2.707180                1.527109            9.656222   \n",
       "\n",
       "   percentile_25  percentile_50  percentile_75  percentile_90  percentile_95  \\\n",
       "0            2.0            2.0       2.666667       4.000000       4.666667   \n",
       "3            2.0            2.0       3.333333       4.000000       6.000000   \n",
       "2            2.0            2.0       3.333333       4.666667       6.000000   \n",
       "1            2.0            2.0       4.000000       4.666667       6.000000   \n",
       "\n",
       "   min       max  \n",
       "0  0.0  8.000000  \n",
       "3  0.0  8.000000  \n",
       "2  0.0  9.333333  \n",
       "1  0.0  8.000000  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neighbors=3\n",
    "weights=\"uniform\"\n",
    "metric=\"euclidean\"\n",
    "nan_filler=-100\n",
    "\n",
    "curr_data_scenarios = {}\n",
    "curr_data_scenarios.update(filename_startswith_data_scenarios)\n",
    "\n",
    "curr_test_data_scenarios = {}\n",
    "curr_test_data_scenarios.update(filename_startswith_test_data_scenarios)\n",
    "\n",
    "# Just a metrics accumulator\n",
    "metrics = []\n",
    "for data_scenario_name, data_scenario in curr_data_scenarios.items():\n",
    "    for test_data_scenario_name, test_data_scenario in curr_test_data_scenarios.items():\n",
    "        curr_data = data_scenario.fillna(nan_filler)\n",
    "        curr_test_data = test_data_scenario.fillna(nan_filler)\n",
    "        curr_metrics = experiment_metrics(knn_experiment(curr_data,\n",
    "                                                         mac_addresses,\n",
    "                                                         [\"x\", \"y\"],\n",
    "                                                         algorithm=\"brute\",\n",
    "                                                         n_neighbors=n_neighbors,\n",
    "                                                         weights=weights,\n",
    "                                                         metric=metric,\n",
    "                                                         test_data=curr_test_data))\n",
    "        curr_metrics[\"data_scenario\"] = data_scenario_name\n",
    "        curr_metrics[\"test_data_scenario\"] = test_data_scenario_name\n",
    "        metrics.append(curr_metrics)\n",
    "\n",
    "cols = [\"data_scenario\", \"test_data_scenario\"] + list(curr_metrics.keys())[:-2]\n",
    "metrics_table = pd.DataFrame(metrics, columns=cols)\n",
    "metrics_table.to_csv(\"metrics-data_scenarios.csv\")\n",
    "metrics_table.sort_values(cols[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Sweeping ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize some variables with the values of each parameter that is going to be swept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_neighbors_values = range(1,2)\n",
    "weights_values = [\"uniform\", \"distance\"]\n",
    "metric_values = [\"euclidean\", \"manhattan\", \"chebyshev\", \"canberra\", \"braycurtis\"]\n",
    "nan_filler_values = [-100.0, -100000.0]\n",
    "units_values = [\"dBm\", \"mW\"]\n",
    "scaler_values = {\"None\": None,\n",
    "                 \"MinMaxScaler\": preprocessing.MinMaxScaler(),\n",
    "                 \"StandardScaler\": preprocessing.StandardScaler(),\n",
    "                 \"RobustScaler\": preprocessing.RobustScaler(),\n",
    "                 \"NormalizerEuclidean\": preprocessing.Normalizer(norm=\"l2\"),\n",
    "                 \"NormalizerManhattan\": preprocessing.Normalizer(norm=\"l1\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the actual parameter sweeping and keep track of the metrics for each parameter combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scenarios = []\n",
    "scenario_keys = None\n",
    "for k_neighbors in k_neighbors_values:\n",
    "    for weights in weights_values:\n",
    "        for metric in metric_values:\n",
    "            for nan_filler in nan_filler_values:\n",
    "                for units in units_values:\n",
    "                    for scaler_name, scaler in scaler_values.items():\n",
    "                        for data_scenario, data in data_scenarios.items():\n",
    "                            for test_data_scenario, test_data in test_data_scenarios.items():\n",
    "                                if k_neighbors < len(data):\n",
    "#                                     print(\"train_data =\", data_scenario)\n",
    "#                                     print(\"test_data =\", test_data_scenario)\n",
    "#                                     print(\"train_data_size =\", len(data))\n",
    "#                                     print(\"test_data_size =\", len(test_data))\n",
    "#                                     print(\"algorithm =\", \"KNeighborsRegressor\")\n",
    "#                                     print(\"n_neighbors =\", k_neighbors)\n",
    "#                                     print(\"weights =\", weights)\n",
    "#                                     print(\"metric =\", metric)\n",
    "#                                     print(\"nan_filler =\", nan_filler)\n",
    "#                                     print(\"units =\", units)\n",
    "#                                     print(\"scaler =\", scaler_name)\n",
    "#                                     print(\"----------------------------------------------------------------\")\n",
    "                                    print(\".\", end='')\n",
    "                                    scenario = collections.OrderedDict([(\"train_data\", data_scenario),\n",
    "                                                                        (\"test_data\", test_data_scenario),\n",
    "                                                                        (\"train_data_size\", len(data)),\n",
    "                                                                        (\"test_data_size\", len(test_data)),\n",
    "                                                                        (\"algorithm\", \"KNeighborsRegressor\"),\n",
    "                                                                        (\"n_neighbors\", k_neighbors),\n",
    "                                                                        (\"weights\", weights),\n",
    "                                                                        (\"metric\", metric),\n",
    "                                                                        (\"nan_filler\", nan_filler),\n",
    "                                                                        (\"units\", units),\n",
    "                                                                        (\"scaler\", scaler_name)])\n",
    "                                    curr_data = data.fillna(nan_filler)\n",
    "                                    curr_test_data = test_data.fillna(nan_filler)\n",
    "                                    curr_data[mac_addresses] = convert_to_units(curr_data[mac_addresses],\n",
    "                                                                                from_units=\"dBm\",\n",
    "                                                                                to_units=units)\n",
    "                                    curr_test_data[mac_addresses] = convert_to_units(curr_test_data[mac_addresses],\n",
    "                                                                                     from_units=\"dBm\",\n",
    "                                                                                     to_units=units)\n",
    "                                    if scaler is not None:\n",
    "                                        scaler.fit(curr_data[mac_addresses])\n",
    "                                        curr_data[mac_addresses] = pd.DataFrame(scaler.transform(curr_data[mac_addresses]),\n",
    "                                                                                columns=mac_addresses)\n",
    "                                        curr_test_data[mac_addresses] = pd.DataFrame(scaler.transform(curr_test_data[mac_addresses]),\n",
    "                                                                                     columns=mac_addresses)\n",
    "                                    scenario.update(experiment_metrics(knn_experiment(curr_data,\n",
    "                                                                                      mac_addresses,\n",
    "                                                                                      [\"x\", \"y\"],\n",
    "                                                                                      algorithm=\"brute\",\n",
    "                                                                                      n_neighbors=k_neighbors,\n",
    "                                                                                      weights=weights,\n",
    "                                                                                      metric=metric,\n",
    "                                                                                      test_data=curr_test_data)))\n",
    "                                    scenario_keys = scenario.keys()\n",
    "                                    scenarios.append(scenario)\n",
    "\n",
    "print(\"\\n\"+str(len(scenarios))+\" scenarios have been simulated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the metrics to disk for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(scenarios, columns=scenario_keys)\n",
    "metrics.to_csv(output_data_directory + \"/metrics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
